{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alan.jeffares/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alan.jeffares/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alan.jeffares/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries}\n",
    "import pandas as pd \n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///data/DisasterResponse.db')\n",
    "df = pd.read_sql('tweets', con=engine)\n",
    "X = df['message']\n",
    "Y = df.drop(columns=['id', 'message', 'original', 'genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am in Carrefour on a street near EDH ( Electricity of Haiti? ) Help us please\n",
      "Everyone, do not forget the people of Merger, we don't have any water, we are hungry. If it rains, we will all die\n",
      "Digicel, I have a problem, my family doesn't know where I am, I can't call, please help\n",
      "Please. Send humanitarian aid so we can survive. We are in Delmas 40B village Musso on the golf course. Thank you\n",
      "good evening we ? carrefour feuill ???we are dying of hunger and ther is no water pleas help us\n",
      "the people in the village of god need food and water so that they can stay. It is isn the university of Kiskeya ( in the area of delams 35 right of the main delmas road ). pleas respond\n",
      "Can someone who has a visa travel out. What can he do?\n",
      "there are alot of victimes that have arrived ath the Marchand Desalin and nan Ranboto hospital. they do not have enough supplys or specialist ( surgeons ) i would like this looked into\n",
      "help us at delmas 65 orchidee street, we are thirsty. thanks\n",
      "we want people to know that people that were in Port au Prince that came to Cap-Haitian have no food to eat. ( Cap-Haitien is in the north, 250 km from Port au Prince )\n",
      "And in St. Marc, just came to save their lives\n",
      "talk to me about the earthquake please.\n",
      "?? port au prince ?? and food. they need goverment aid and international aid thak you. god bless haiti\n",
      "?? port au prince ?? and food. they need goverment aid and international aid thak you. god bless haiti\n",
      "I am in Liancourt a province in the artibonit. we have recieved alot of victimes from pap. in different condition. some are very serious some are not. pleas send help for these people.\n",
      "Hello, we live in La Plaine, close to Route National #1, between Carrefour Vencent and bary zone. We need food, water, tents etc. ..\n",
      "we are in leogane mathieu abitation 3rd communal section grand riviere we are waiting.\n",
      "We are at the top of Fontamara 43 (area of Carrefour), at the top of the hill named label.. unrecognized characters. We did not get any assistance: water, food, tents, medicine.\n",
      "The medicine should be distributed in all the departments ( counties ) because the hospital ( s ). ..\n",
      "One thing I am asking the money tranfer offices is for them to open so we can get the money sent to us.\n",
      "PLEASE, We are in Nazon, Sylvio Cator street. We need water, food and other truncated\n",
      "Good morning, to everyone that is listening in Miami and other countries helping. I have my wife and five kids that will starve to death in Haiti if they do not get help. PLease help them! God will bless you!\n",
      "We are in Delmas 95. We need food, water, clothes. Our house was destroyed.\n",
      "Good afternoon, we are a group in Bon Repo in Loubens street. We need help in every sense of the word, but nobody is thinking of us. Please do something for us. Bye\n",
      "Please help us! We are in Primati Malgro. They gave them stuff but we did not get any. It's like you have to fight get something. We are hungry, thirsty and we are suffering. Please help us!\n",
      "I know of an internet service in Croix des Bouquets, Falaise Street. The more important would be to open wireless zones for everyone that has laptops\n",
      "There's a group of people under the rubles at Village Solidarite\n",
      "I am in Delmas there is a problem with water\n",
      "Send me some minutes for my phone\n",
      "We are at Lycee Philippe Guerrier at les Cayes. We had to leave Port-au-Prince because of the bad odor and conditions of life in the capital. We need help please.\n",
      "Watch out! This is a warning! There is a bad foul smell coming from the well that is lingering in the area. Near the CHurch of God near the yard of Woolio Nway's house!\n",
      "I live in St. Louis. I have some problems. Can take me some place because I have problems in Port au Prince. Thank you for your help!\n",
      "There are many people badly wounded who had to go who can not eat.\n",
      "Noone has come to visit us in Delmas A1. We need food and water!\n",
      "go to delmas 1 near goldstar in painson s. a yard - we need help\n",
      "We are in Jean Rabel, we have nothing to eat. Please come to us because we were in Port-au-Prince, we lost everything\n",
      "we have a lot of problems at Cap Haitian. The Hospital cannot take anymore people. Come help us please\n",
      "I need help rescuing someone under the rubble at the Caraibean University, Delmas 29 #5.\n",
      "Please, we need humanitarian aid because the Lwes region has been hit badly.\n",
      "Good evening, please come quickly to assist homeless victims who are in village of Morin, section of Taifer, municipality of K-Fou Carrefour. Thank you.\n"
     ]
    }
   ],
   "source": [
    "a = 150\n",
    "for i in range(a, a+40):\n",
    "    print(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['un',\n",
       " 'report',\n",
       " 'leogane',\n",
       " '80-90',\n",
       " 'destroyed',\n",
       " '.',\n",
       " 'only',\n",
       " 'hospital',\n",
       " 'st.',\n",
       " 'croix',\n",
       " 'functioning',\n",
       " '.',\n",
       " 'needs',\n",
       " 'supply',\n",
       " 'desperately',\n",
       " '.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X[3])\n",
    "tokenize(X[3])\n",
    "\n",
    "# tokens = word_tokenize(X[0])\n",
    "# lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok).lower().strip() for tok in tokens]\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', (MLPClassifier()))\n",
    "    ])\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#         ('features', FeatureUnion([\n",
    "\n",
    "#             ('text_pipeline', Pipeline([\n",
    "#                 ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "#                 ('tfidf', TfidfTransformer())\n",
    "#             ])),\n",
    "\n",
    "#             ('starting_verb', StartingVerbExtractor())\n",
    "#         ])),\n",
    "\n",
    "#         ('clf', DecisionTreeClassifier())\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/disaster_response_env/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('vect',\n",
       "                                                                  CountVectorizer(analyzer='word',\n",
       "                                                                                  binary=False,\n",
       "                                                                                  decode_error='strict',\n",
       "                                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                                  encoding='utf-8',\n",
       "                                                                                  input='content',\n",
       "                                                                                  lowercase=True,\n",
       "                                                                                  max_df=1.0,\n",
       "                                                                                  max_features=None,\n",
       "                                                                                  min_df=1,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               1),\n",
       "                                                                                  preprocessor=Non...\n",
       "                               batch_size='auto', beta_1=0.9, beta_2=0.999,\n",
       "                               early_stopping=False, epsilon=1e-08,\n",
       "                               hidden_layer_sizes=(100,),\n",
       "                               learning_rate='constant',\n",
       "                               learning_rate_init=0.001, max_iter=200,\n",
       "                               momentum=0.9, n_iter_no_change=10,\n",
       "                               nesterovs_momentum=True, power_t=0.5,\n",
       "                               random_state=None, shuffle=True, solver='adam',\n",
       "                               tol=0.0001, validation_fraction=0.1,\n",
       "                               verbose=False, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.1)\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipeline.predict(X_test)\n",
    "Y_pred = pd.DataFrame(Y_pred, columns=Y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/disaster_response_env/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy     0.937172\n",
       "precision    0.721678\n",
       "recall       0.578209\n",
       "dtype: float64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "for column_name in Y_pred.columns:\n",
    "    col_report = classification_report(y_true=Y_test[[column_name]], y_pred=Y_pred[[column_name]], output_dict=True)\n",
    "    accuracy = col_report['accuracy']\n",
    "    precision = col_report['macro avg']['precision']\n",
    "    recall = col_report['macro avg']['recall']\n",
    "    results[column_name] = [accuracy, precision, recall]\n",
    "results.index = ['accuracy', 'precision', 'recall']\n",
    "results.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'features__text_pipeline__vect__max_df': (0.5, 1.0),\n",
    "        'features__text_pipeline__vect__min_df': (1, 0.01),\n",
    "        'features__text_pipeline__vect__max_features': (None, 5000),\n",
    "        'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf': (DecisionTreeClassifier(min_samples_split=3),),\n",
    "        'clf__max_depth': (None, 4)\n",
    "    }, {\n",
    "        'features__text_pipeline__vect__max_df': (0.5, 1.0),\n",
    "        'features__text_pipeline__vect__min_df': (1, 0.01),\n",
    "        'features__text_pipeline__vect__max_features': (None, 5000),\n",
    "        'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf': (MultiOutputClassifier(LinearSVC(multi_class='ovr')),)\n",
    "    }, {\n",
    "        'features__text_pipeline__vect__max_df': (0.5, 1.0),\n",
    "        'features__text_pipeline__vect__min_df': (1, 0.01),\n",
    "        'features__text_pipeline__vect__max_features': (None, 5000),\n",
    "        'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf': (MLPClassifier(),),\n",
    "        'clf__hidden_layer_sizes': ((100, 10), (50,), (50, 10))\n",
    "    }\n",
    "]\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=4, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20972,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=4)]: Done 288 out of 288 | elapsed: 22.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('vect',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         m...\n",
       "                                                warm_start=False),),\n",
       "                          'clf__hidden_layer_sizes': ((100, 50), (50,),\n",
       "                                                      (50, 25)),\n",
       "                          'features__text_pipeline__tfidf__use_idf': (True,\n",
       "                                                                      False),\n",
       "                          'features__text_pipeline__vect__max_df': (0.5, 1.0),\n",
       "                          'features__text_pipeline__vect__max_features': (None,\n",
       "                                                                          5000),\n",
       "                          'features__text_pipeline__vect__min_df': (1, 0.01)}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = cv.predict(X_test)\n",
    "Y_pred = pd.DataFrame(Y_pred, columns=Y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy     0.948745\n",
       "precision    0.891526\n",
       "recall       0.626660\n",
       "dtype: float64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "for column_name in Y_pred.columns:\n",
    "    col_report = classification_report(y_true=Y_test[[column_name]], y_pred=Y_pred[[column_name]], output_dict=True)\n",
    "    accuracy = col_report['accuracy']\n",
    "    precision = col_report['macro avg']['precision']\n",
    "    recall = col_report['macro avg']['recall']\n",
    "    results[column_name] = [accuracy, precision, recall]\n",
    "results.index = ['accuracy', 'precision', 'recall']\n",
    "results.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_header = not os.path.isfile('Model_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff6179bb8def>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mavg_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mavg_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavg_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = type(cv.best_params_['clf']).__name__\n",
    "avg_accuracy = results.mean(axis=1)['accuracy']\n",
    "avg_precision = results.mean(axis=1)['precision']\n",
    "avg_recall = results.mean(axis=1)['recall']\n",
    "params = cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_results = pd.DataFrame({'Model': [model_name], 'Accuracy': [avg_accuracy], 'Precision': [avg_precision], \n",
    "                               'Recall': [avg_recall], 'Parameters': [params]})\n",
    "\n",
    "add_header = not os.path.isfile('Model_results.csv')\n",
    "\n",
    "with open('Model_results.csv', 'a') as f:\n",
    "    stored_results.to_csv(f, header=add_header, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27/10/19 15:02</th>\n",
       "      <td>MultiOutputClassifier</td>\n",
       "      <td>0.948745</td>\n",
       "      <td>0.891526</td>\n",
       "      <td>0.62666</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Linear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model  Accuracy  Precision   Recall  \\\n",
       "27/10/19 15:02  MultiOutputClassifier  0.948745   0.891526  0.62666   \n",
       "\n",
       "                                                       Parameters  \n",
       "27/10/19 15:02  {'clf': MultiOutputClassifier(estimator=Linear...  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>clf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>MultiOutputClassifier(estimator=LinearSVC(C=1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>MultiOutputClassifier(estimator=LinearSVC(C=1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2</td>\n",
       "      <td>MultiOutputClassifier(estimator=LinearSVC(C=1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>MultiOutputClassifier(estimator=LinearSVC(C=1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>MultiOutputClassifier(estimator=LinearSVC(C=1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>MLPClassifier(activation='relu', alpha=0.0001,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>93</td>\n",
       "      <td>MLPClassifier(activation='relu', alpha=0.0001,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>94</td>\n",
       "      <td>MLPClassifier(activation='relu', alpha=0.0001,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>95</td>\n",
       "      <td>MLPClassifier(activation='relu', alpha=0.0001,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>95</td>\n",
       "      <td>MLPClassifier(activation='relu', alpha=0.0001,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank                                                clf\n",
       "40     1  MultiOutputClassifier(estimator=LinearSVC(C=1....\n",
       "34     2  MultiOutputClassifier(estimator=LinearSVC(C=1....\n",
       "46     2  MultiOutputClassifier(estimator=LinearSVC(C=1....\n",
       "42     4  MultiOutputClassifier(estimator=LinearSVC(C=1....\n",
       "44     5  MultiOutputClassifier(estimator=LinearSVC(C=1....\n",
       "..   ...                                                ...\n",
       "92    92  MLPClassifier(activation='relu', alpha=0.0001,...\n",
       "54    93  MLPClassifier(activation='relu', alpha=0.0001,...\n",
       "52    94  MLPClassifier(activation='relu', alpha=0.0001,...\n",
       "84    95  MLPClassifier(activation='relu', alpha=0.0001,...\n",
       "86    95  MLPClassifier(activation='relu', alpha=0.0001,...\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = pd.Series(cv.cv_results_['rank_test_score'])\n",
    "clf = pd.Series(cv.cv_results_['param_clf'])\n",
    "results = pd.DataFrame({'rank':rank, 'clf': clf})\n",
    "results.sort_values('rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl','wb') as f:\n",
    "    pickle.dump(cv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c6057e3c3834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/classifier.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_utils'"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open(\"models/classifier.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster_response_env",
   "language": "python",
   "name": "disaster_response_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
